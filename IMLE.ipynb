{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vb0DM2v80jN2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# --------------------------------------------------------\n",
        "# References:\n",
        "# GLIDE: https://github.com/openai/glide-text2im\n",
        "# MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
        "\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#               Embedding Layers for Timesteps and Class Labels                 #\n",
        "#################################################################################\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds scalar timesteps into vector representations.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
        "        )\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    # @staticmethod\n",
        "    # def timestep_embedding(t, dim, max_period=10000):\n",
        "    #     \"\"\"\n",
        "    #     Create sinusoidal timestep embeddings.\n",
        "    #     :param t: a 1-D Tensor of N indices, one per batch element.\n",
        "    #                       These may be fractional.\n",
        "    #     :param dim: the dimension of the output.\n",
        "    #     :param max_period: controls the minimum frequency of the embeddings.\n",
        "    #     :return: an (N, D) Tensor of positional embeddings.\n",
        "    #     \"\"\"\n",
        "    #     # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
        "    #     half = dim // 2\n",
        "    #     freqs = torch.exp(\n",
        "    #         -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    #     ).to(device=t.device)\n",
        "    #     args = t[:, None].float() * freqs[None]\n",
        "    #     embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    #     if dim % 2:\n",
        "    #         embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    #     return embedding\n",
        "\n",
        "    # def forward(self, t):\n",
        "    #     t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
        "    #     t_emb = self.mlp(t_freq)\n",
        "    #     return t_emb\n",
        "\n",
        "    def forward(self, x, t=None, y=None):\n",
        "        \"\"\"\n",
        "        Forward pass for DiT without diffusion steps.\n",
        "        x: (N, C, H, W) tensor of spatial inputs.\n",
        "        \"\"\"\n",
        "        x = self.x_embedder(x) + self.pos_embed  # Only use spatial input\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c=None)  # Remove conditional embeddings\n",
        "        x = self.final_layer(x, c=None)  # Pass through final layer\n",
        "        x = self.unpatchify(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LabelEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
        "        super().__init__()\n",
        "        use_cfg_embedding = dropout_prob > 0\n",
        "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def token_drop(self, labels, force_drop_ids=None):\n",
        "        \"\"\"\n",
        "        Drops labels to enable classifier-free guidance.\n",
        "        \"\"\"\n",
        "        if force_drop_ids is None:\n",
        "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
        "        else:\n",
        "            drop_ids = force_drop_ids == 1\n",
        "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
        "        return labels\n",
        "\n",
        "    def forward(self, labels, train, force_drop_ids=None):\n",
        "        use_dropout = self.dropout_prob > 0\n",
        "        if (train and use_dropout) or (force_drop_ids is not None):\n",
        "            labels = self.token_drop(labels, force_drop_ids)\n",
        "        embeddings = self.embedding_table(labels)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                 Core DiT Model                                #\n",
        "#################################################################################\n",
        "\n",
        "class DiTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
        "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
        "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c=None):  # Conditional input c is now optional\n",
        "        x = x + self.attn(self.norm1(x))  # Skip conditioning\n",
        "        x = x + self.mlp(self.norm2(x))  # Skip conditioning\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    The final layer of DiT.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x, c):\n",
        "    #     shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "    #     x = modulate(self.norm_final(x), shift, scale)\n",
        "    #     x = self.linear(x)\n",
        "    #     return x\n",
        "    def forward(self, x, c=None):  # Conditional input c is now optional\n",
        "        x = self.norm_final(x)  # Skip conditional modulation\n",
        "        x = self.linear(x)  # Directly map to output\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiT(nn.Module):\n",
        "    \"\"\"\n",
        "    Diffusion model with a Transformer backbone.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=32,\n",
        "        patch_size=2,\n",
        "        in_channels=4,\n",
        "        hidden_size=1152,\n",
        "        depth=28,\n",
        "        num_heads=16,\n",
        "        mlp_ratio=4.0,\n",
        "        class_dropout_prob=0.1,\n",
        "        num_classes=1000,\n",
        "        learn_sigma=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.learn_sigma = learn_sigma\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
        "        self.t_embedder = None\n",
        "        self.y_embedder = None\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        # Will use fixed sin-cos embedding:\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.proj.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size**2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels\n",
        "        p = self.x_embedder.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "    \n",
        "    def forward(self, x, t=None, y=None):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT without timestep embeddings.\n",
        "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images).\n",
        "        \"\"\"\n",
        "        # Spatial embedding with positional information\n",
        "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "\n",
        "        # No timestep or class embeddings; c is set to None\n",
        "        c = None\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)  # Blocks are adjusted to handle c=None\n",
        "\n",
        "        # Final layer processing\n",
        "        x = self.final_layer(x, c)  # Final layer adjusted to handle c=None\n",
        "        x = self.unpatchify(x)  # Convert back to spatial format (N, out_channels, H, W)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward_with_cfg(self, x, t=None, y=None, cfg_scale=1.0):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT without timestep embeddings, with support for classifier-free guidance.\n",
        "        \"\"\"\n",
        "        # Split input into halves and duplicate for guidance\n",
        "        half = x[: len(x) // 2]\n",
        "        combined = torch.cat([half, half], dim=0)\n",
        "\n",
        "        # Perform forward pass without using timesteps\n",
        "        model_out = self.forward(combined, t=None, y=y)\n",
        "\n",
        "        # Apply classifier-free guidance\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]  # Split output into guided parts\n",
        "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
        "\n",
        "        # Recombine outputs\n",
        "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "        return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                   Sine/Cosine Positional Embedding Functions                  #\n",
        "#################################################################################\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token and extra_tokens > 0:\n",
        "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                                   DiT Configs                                  #\n",
        "#################################################################################\n",
        "\n",
        "def DiT_XL_2(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_4(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_XL_8(**kwargs):\n",
        "    return DiT(depth=28, hidden_size=1152, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_2(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_4(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=4, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_L_8(**kwargs):\n",
        "    return DiT(depth=24, hidden_size=1024, patch_size=8, num_heads=16, **kwargs)\n",
        "\n",
        "def DiT_B_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=4, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_B_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=768, patch_size=8, num_heads=12, **kwargs)\n",
        "\n",
        "def DiT_S_2(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_4(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=4, num_heads=6, **kwargs)\n",
        "\n",
        "def DiT_S_8(**kwargs):\n",
        "    return DiT(depth=12, hidden_size=384, patch_size=8, num_heads=6, **kwargs)\n",
        "\n",
        "\n",
        "DiT_models = {\n",
        "    'DiT-XL/2': DiT_XL_2,  'DiT-XL/4': DiT_XL_4,  'DiT-XL/8': DiT_XL_8,\n",
        "    'DiT-L/2':  DiT_L_2,   'DiT-L/4':  DiT_L_4,   'DiT-L/8':  DiT_L_8,\n",
        "    'DiT-B/2':  DiT_B_2,   'DiT-B/4':  DiT_B_4,   'DiT-B/8':  DiT_B_8,\n",
        "    'DiT-S/2':  DiT_S_2,   'DiT-S/4':  DiT_S_4,   'DiT-S/8':  DiT_S_8,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQBihZae1HD0"
      },
      "source": [
        "### Generator\n",
        "Here we define a simple generator consisting of a few fully connected layers. We initial its weight using standard Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ml0zmts-0rgG"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    def truncated_normal_init(t, mean=0.0, std=0.5):\n",
        "        torch.nn.init.normal_(t, mean=mean, std=std)\n",
        "        while True:\n",
        "            cond = torch.logical_or(t < mean - 2 * std, t > mean + 2 * std)\n",
        "            if not torch.sum(cond):\n",
        "                break\n",
        "            t = torch.where(cond, torch.nn.init.normal_(torch.ones(t.shape), mean=mean, std=std), t)\n",
        "        return t\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_mlp=5, in_dim=32, out_dim=2, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        assert(n_mlp >= 2)\n",
        "\n",
        "        layers = [nn.Linear(in_dim, hidden_dim), nn.LeakyReLU(0.2)]\n",
        "        for i in range(n_mlp - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.Sigmoid())\n",
        "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, latents):\n",
        "        return self.layers(latents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJnVlRKP1EL4"
      },
      "source": [
        "### Plotting function\n",
        "We define a simple plotting function to visualize data examples and generated examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RxRwOk480_7H"
      },
      "outputs": [],
      "source": [
        "data_point_size = 120\n",
        "nn_point_size = 120\n",
        "generated_point_size = 120\n",
        "legend_fontsize = 12\n",
        "frameon = True\n",
        "figsize = 10\n",
        "\n",
        "def plot_kde(generated, data_points, f_name, title):\n",
        "    plt.clf()\n",
        "    data = pd.DataFrame.from_dict({'x': generated[:, 0], 'y': generated[:, 1]})\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.ylim(-0.3, 0.4)\n",
        "    plt.xlim(-0.35, 0.35)\n",
        "    plt.title(title)\n",
        "    plt.scatter(data_points[:, 0], data_points[:, 1], label=\"Real data points\", color=\"blue\")\n",
        "    sns.kdeplot(data=data, fill=True, levels=200, alpha=0.5, palette=\"rocket_r\")\n",
        "    plt.savefig(f_name)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def plot_nns(g, data_points, zs, selected_z, title=None, f_name=None, arrow=False, vanilla=False):\n",
        "    generated = g(zs).detach()\n",
        "    if(not vanilla):\n",
        "        selected = g(selected_z).detach()\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    # plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
        "    plt.scatter(data_points[:, 0], data_points[:, 1], label=\"Real data points\", color=\"blue\", marker=\"s\", s=data_point_size)\n",
        "    plt.scatter(generated[:, 0], generated[:, 1], label=\"Generated samples\", color=\"orange\", alpha=0.6, marker=\".\", s=generated_point_size)\n",
        "    if(not vanilla):\n",
        "        plt.scatter(selected[:, 0], selected[:, 1], label=\"Nearest neighbours\", color=\"red\",marker=\"*\", s=nn_point_size)\n",
        "\n",
        "    if arrow:\n",
        "        generated_nn = g(selected_z).detach()\n",
        "        gen_to_data_vec = data_points - generated_nn\n",
        "        # unit_vecs = gen_to_data_vec / np.linalg.norm(gen_to_data_vec, axis=1)[:, None]\n",
        "        plt.quiver(generated_nn[:, 0], generated_nn[:, 1], 0.19*gen_to_data_vec[:, 0], 0.19*gen_to_data_vec[:, 1],\n",
        "                   color=\"black\", scale_units=\"xy\", angles='xy', scale=0.2, width=0.0015, headwidth=10, headlength=10)\n",
        "\n",
        "    plt.legend(fontsize = legend_fontsize, frameon=frameon, loc=\"upper right\")\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.ylim(-1.4, 1.4)\n",
        "    plt.xlim(-1.4, 1.4)\n",
        "    plt.axis('off')\n",
        "    if f_name:\n",
        "        plt.savefig(f_name, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "        #plot_kde(generated, data_points, f_name.replace(\".png\", \"_kde.png\"), title)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_nns_eps(g, data_points, zs, selected_z, title=None, f_name=None, arrow=False, vanilla=False, eps=0.1):\n",
        "    generated = g(zs).detach()\n",
        "    if(not vanilla):\n",
        "        selected = g(selected_z).detach()\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    # plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
        "    plt.scatter(data_points[:, 0], data_points[:, 1], label=\"Real data points\", color=\"blue\", marker=\"s\", s=data_point_size)\n",
        "    plt.scatter(generated[:, 0], generated[:, 1], label=\"Generated samples\", color=\"orange\", alpha=0.5, marker=\".\", s=generated_point_size)\n",
        "\n",
        "    for center in data_points:\n",
        "        circle = plt.Circle(center, eps, fill=True, alpha=0.2, linestyle=\"--\", edgecolor=\"black\", facecolor=\"gray\")\n",
        "        plt.gca().add_patch(circle)\n",
        "\n",
        "    if(not vanilla):\n",
        "        plt.scatter(selected[:, 0], selected[:, 1], label=\"Nearest neighbours\", color=\"red\",marker=\"*\", s=nn_point_size)\n",
        "\n",
        "    if arrow:\n",
        "        generated_nn = g(selected_z).detach()\n",
        "        gen_to_data_vec = data_points - generated_nn\n",
        "        # unit_vecs = gen_to_data_vec / np.linalg.norm(gen_to_data_vec, axis=1)[:, None]\n",
        "        plt.quiver(generated_nn[:, 0], generated_nn[:, 1], 0.19*gen_to_data_vec[:, 0], 0.19*gen_to_data_vec[:, 1],\n",
        "                   color=\"black\", scale_units=\"xy\", angles='xy', scale=0.2, width=0.0015, headwidth=10, headlength=10)\n",
        "\n",
        "    # plt.legend(fontsize = legend_fontsize, frameon=frameon, loc=\"lower left\", mode = \"expand\", ncol = 3)\n",
        "    plt.legend(fontsize = legend_fontsize, frameon=frameon, loc=\"upper right\")\n",
        "\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.ylim(-1.4, 1.4)\n",
        "    plt.xlim(-1.4, 1.4)\n",
        "    plt.axis('off')\n",
        "\n",
        "    if f_name:\n",
        "        plt.savefig(f_name, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "        #plot_kde(generated, data_points, f_name.replace(\".png\", \"_kde.png\"), title)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_final(g, data_points, zs, selected_z, title=None, f_name=None, arrow=False, vanilla=False):\n",
        "    generated = g(zs).detach()\n",
        "    if(not vanilla):\n",
        "        selected = g(selected_z).detach()\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    # plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
        "    # plt.scatter(data_points[:, 0], data_points[:, 1], label=\"Real data points\", color=\"blue\", marker=\"s\")\n",
        "    plt.scatter(generated[:, 0], generated[:, 1], label=\"Generated samples\", color=\"orange\", alpha=0.5, marker=\".\")\n",
        "    if(not vanilla):\n",
        "        plt.scatter(selected[:, 0], selected[:, 1], label=\"Nearest neighbours\", color=\"red\",marker=\".\")\n",
        "\n",
        "    if arrow:\n",
        "        gen_to_data_vec = data_points - generated\n",
        "        # unit_vecs = gen_to_data_vec / np.linalg.norm(gen_to_data_vec, axis=1)[:, None]\n",
        "        plt.quiver(generated[:, 0], generated[:, 1], 0.19*gen_to_data_vec[:, 0], 0.29*gen_to_data_vec[:, 1],\n",
        "                   color=\"black\", scale_units=\"xy\", angles='xy', scale=0.2, width=0.0015, headwidth=10, headlength=10, label=\"Nearest neighbour vectors\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.ylim(-1.2, 1.2)\n",
        "    plt.xlim(-1.2, 1.2)\n",
        "    if f_name:\n",
        "        plt.savefig(f_name)\n",
        "        plt.close()\n",
        "        #plot_kde(generated, data_points, f_name.replace(\".png\", \"_kde.png\"), title)\n",
        "    else:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5KOsqLH1Lah"
      },
      "source": [
        "# Dataset\n",
        "Here we consider some 2D data examples defined by _data\\_points_. We create a generator object and visualize some generated samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "BCmd0vw71A65",
        "outputId": "c85cf37a-15d6-43ff-bb2c-b77ef923e76f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3278640/1640475039.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_points = torch.load(\"smiley.pt\")\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'torch.Size' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generator(n_mlp=4, in_dim=zdim, hidden_dim=32)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m data_points \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiley.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdata_points\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m data_points \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(data_points, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m data_points \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m7\u001b[39m])\n",
            "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
          ]
        }
      ],
      "source": [
        "zdim = 32  # latent space dimension\n",
        "g = DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6) \n",
        "\n",
        "# Generator(n_mlp=4, in_dim=zdim, hidden_dim=32)\n",
        "\n",
        "data_points = torch.load(\"smiley.pt\")\n",
        "print(data_points.shape)\n",
        "\n",
        "data_points -= torch.mean(data_points, 0)\n",
        "data_points /= torch.tensor([10,7])\n",
        "\n",
        "n = data_points.shape[0]\n",
        "nz = data_points.shape[0]*10  # pool size\n",
        "zs = torch.randn(nz, zdim)\n",
        "\n",
        "plot_nns(g, data_points, zs, None, title=\"Generated samples\", vanilla=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cdWUj221Ybp"
      },
      "source": [
        "### Hyperparameters\n",
        "Here we defined a few hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1zMei9rP1TYa"
      },
      "outputs": [],
      "source": [
        "lr = 0.01  # learning rate\n",
        "epochs = 10000  # number of epochs for training\n",
        "tau=0.85  # tightening threshold defined in the Adaptive IMLE paper\n",
        "noise_coef = 0.000001  # additive noise coeficient\n",
        "staleness = 1000  # staleness for Vanilla IMLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2swrNM9z1cFq"
      },
      "source": [
        "### Nearest Neighbours\n",
        "Here we define a naive function to find nearest neighbour of each data example. This nearest neighbour part can be vastly optimized using [DCI](https://arxiv.org/abs/1703.00440)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hxlwxT1E1dew"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     dists \u001b[38;5;241m=\u001b[39m dists\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmin(dists)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m----> 6\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      7\u001b[0m nns \u001b[38;5;241m=\u001b[39m [find_nn(data_points[i], generated) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n",
            "File \u001b[0;32m~/miniconda3/envs/rs-imle/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/rs-imle/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[3], line 247\u001b[0m, in \u001b[0;36mDiT.forward\u001b[0;34m(self, x, t, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03mForward pass of DiT without timestep embeddings.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03mx: (N, C, H, W) tensor of spatial inputs (images or latent representations of images).\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Spatial embedding with positional information\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed  \u001b[38;5;66;03m# (N, T, D), where T = H * W / patch_size ** 2\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# No timestep or class embeddings; c is set to None\u001b[39;00m\n\u001b[1;32m    250\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/rs-imle/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/rs-imle/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/rs-imle/lib/python3.10/site-packages/timm/layers/patch_embed.py:113\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 113\u001b[0m     B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict_img_size:\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ],
      "source": [
        "def find_nn(data_point, generated):\n",
        "    dists = torch.sum((generated - data_point)**2, dim=1)\n",
        "    dists = dists**0.5\n",
        "    return torch.argmin(dists).item()\n",
        "\n",
        "generated = g(zs).detach()\n",
        "nns = [find_nn(data_points[i], generated) for i in range(n)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exKNZ4JI1iUh"
      },
      "source": [
        "### Vanilla IMLE implementation\n",
        "Here we implement a basic version of Vanilla IMLE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6WP1pqi1gvD",
        "outputId": "0fef3149-a376-4121-ebc0-b5aa676e1e14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [01:38<00:00, 305.30it/s]\n"
          ]
        }
      ],
      "source": [
        "imle_g = copy.deepcopy(g)\n",
        "optim = torch.optim.Adam(imle_g.parameters(), lr=lr)\n",
        "imle_nn_z = torch.randn(n, zdim)\n",
        "epochs = 30000\n",
        "staleness = 20\n",
        "plot_staleness = 5000\n",
        "factor = 10\n",
        "nz = data_points.shape[0]*factor\n",
        "\n",
        "if(not os.path.exists(\"plots_smiley\")):\n",
        "    os.makedirs(\"plots_smiley\")\n",
        "\n",
        "\n",
        "for e in tqdm(range(epochs)):\n",
        "    with torch.no_grad():\n",
        "        if e % staleness == 0:\n",
        "            zs = torch.randn(nz, zdim)\n",
        "            generated = imle_g(zs).detach()\n",
        "\n",
        "            ## find_nn is extremely slow. You can use KNN search here.\n",
        "            ## Batch size is too large. You can use smaller batch size.\n",
        "            nns = torch.tensor([find_nn(d, generated) for d in data_points], dtype=torch.long)\n",
        "            imle_nn_z = zs[nns] + torch.randn(nns.shape[0], zdim) * noise_coef\n",
        "        if e % plot_staleness == 0:\n",
        "            plot_nns(imle_g, data_points, zs, imle_nn_z, title=f\"IMLE Procedure\", f_name=f\"plots_smiley/imle-epoch-{e}.png\", arrow=True, vanilla=False)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    outs = imle_g(imle_nn_z)\n",
        "    # dists = torch.sqrt(torch.sum((outs - data_points)**2, dim=1))\n",
        "    dists = torch.sum((outs - data_points)**2, dim=1)\n",
        "\n",
        "    loss = dists.mean()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "\n",
        "nz_fake = data_points.shape[0]*40\n",
        "zs = torch.randn(nz_fake, zdim)\n",
        "plot_final(imle_g, data_points, zs, None, title=\"Vanilla IMLE final\", f_name=f\"plots_smiley/imle-final.png\", arrow=False, vanilla=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyM40rhTJDDo"
      },
      "source": [
        "### RS IMLE implementation\n",
        "Here we implement Rejection Sampling IMLE. We reject samples that are too close to a data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pyi3O8uI8hJ"
      },
      "outputs": [],
      "source": [
        "def find_nn_rsimle(data_point, generated, limit = 1e-3,print_results=False):\n",
        "    l2_distances = np.linalg.norm(generated[:, np.newaxis] - data_point , axis=2)\n",
        "    # l2_distances = l2_distances**2\n",
        "    to_exclude = l2_distances < limit\n",
        "    bad_samples_list = np.expand_dims(to_exclude.any(axis=1),-1)\n",
        "    bad_samples_list_repeated = np.tile(bad_samples_list,to_exclude.shape[1])\n",
        "    l2_distances[bad_samples_list_repeated] = np.inf\n",
        "    result = np.argmin(l2_distances,axis=0)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIpQ-PQ1JT52",
        "outputId": "c2028db2-7730-48cb-8d56-2214a8bf746c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [01:38<00:00, 305.74it/s]\n"
          ]
        }
      ],
      "source": [
        "rsimle_g = copy.deepcopy(g)\n",
        "optim = torch.optim.Adam(rsimle_g.parameters(), lr=lr)\n",
        "imle_nn_z = torch.randn(n, zdim)\n",
        "epochs = 30000\n",
        "staleness = 20\n",
        "plot_staleness = 5000\n",
        "factor = 10\n",
        "nz = data_points.shape[0]*factor\n",
        "\n",
        "## Epsilon to reject samples\n",
        "distance_threshold = 2e-2\n",
        "\n",
        "for e in tqdm(range(epochs)):\n",
        "    with torch.no_grad():\n",
        "        if e % staleness == 0:\n",
        "            zs = torch.randn(nz, zdim)\n",
        "            generated = rsimle_g(zs).detach()\n",
        "            nns = torch.tensor(find_nn_rsimle(data_points, generated, limit=distance_threshold), dtype=torch.long)\n",
        "            imle_nn_z = zs[nns] + torch.randn(nns.shape[0], zdim) * noise_coef\n",
        "\n",
        "        if e % plot_staleness == 0:\n",
        "            plot_nns_eps(rsimle_g, data_points, zs, imle_nn_z, title=f\"RS-IMLE Procedure\", f_name=f\"plots_smiley/rsimle-epoch-{e}.png\", arrow=True, vanilla=False, eps=distance_threshold)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    outs = rsimle_g(imle_nn_z)\n",
        "    # dists = torch.sqrt(torch.sum((outs - data_points)**2, dim=1))\n",
        "    dists = torch.sum((outs - data_points)**2, dim=1)\n",
        "    loss = dists.mean()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "nz_fake = data_points.shape[0]*40\n",
        "zs = torch.randn(nz_fake, zdim)\n",
        "plot_final(rsimle_g, data_points, zs, None, title=\"RS-IMLE final\", f_name=f\"plots_smiley/rsimle-final-{distance_threshold}.png\", arrow=False, vanilla=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rs-imle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
